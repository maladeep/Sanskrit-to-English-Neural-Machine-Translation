# Sanskrit-to-English-Neural-Machine-Translation


In this project, we employed and explored two distinct sequence-to-sequence modeling approaches: one based on Recurrent Neural Networks (RNN) with an encoder-decoder architecture with an attention mechanism, and the other based on the state-of-the-art Transformer model. We aimed to compare and contrast the performance of these two methods in a sequence-to-sequence task.


## The primary objectives of this project are as follows:

1.	Compare the machine translation methods like the encoder-decoder RNN architecture and the transformer architecture to solve the sequence-to-sequence (seq2seq) learning problem.
2.	Develop a robust neural machine translation platform using machine learning techniques, with a specific emphasis on neural networks and Python.
3.	Enhance the system's accuracy and adaptability through feature engineering and integration of deep learning methodologies and optimize the model by addressing data leakage, and hyperparameter tuning. 


## Installation
1. Clone the repository:
   
   `git clone https://github.com/maladeep/Sanskrit-to-English-Neural-Machine-Translation.git`

2. Install the required dependencies:
   
   `pip install -r requirements.txt`


## Contributing

Contributions to this project are welcome. If you find any issues or would like to suggest improvements, please open an issue or submit a pull request. 

## License

This project is licensed under the MIT License. See the LICENSE file for more information.
